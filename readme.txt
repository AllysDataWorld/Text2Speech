However, since these Integer IDs (or their corresponding one-hot encoded vectors) are
assigned randomly to words, they lack any inherent semantic meaning. This is where
embeddings are much more useful. Although itâ€™s possible to embed character and sub-word
level tokens as well, let us look at word and document embeddings to understand some of
the methods behind them.